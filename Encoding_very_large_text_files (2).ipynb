{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoding very large text files",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_SqIOlD8yd7"
      },
      "source": [
        "# Optional: downloading training texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpeJ3Wup82qC"
      },
      "source": [
        "!pip install dtrx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQuu91lr_j4E"
      },
      "source": [
        "!lang=\"vi\"; wget -c https://dumps.wikimedia.org/${lang}wiki/latest/${lang}wiki-latest-pages-articles.xml.bz2 & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-wikimedia/v20210402/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-CCAligned/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-XLEnt/v1.1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-JW300/v1b/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-TED2020/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-GNOME/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-bible-uedin/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-ParaCrawl/v8/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-TildeMODEL/v2018/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-DGT/v2019/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-EUbookshop/v2/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-SETIMES/v2/mono/${lang}.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPvrtGy9AW5n"
      },
      "source": [
        "!dtrx *.txt.gz.* & dtrx *.txt.gz & dtrx *.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFasaOzpBCrT"
      },
      "source": [
        "!cat *.txt > dataset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMWzd0QS-c4h"
      },
      "source": [
        "# Downloading and cleaning Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhH-lihUBIl5"
      },
      "source": [
        "!nohup curl -L -O https://dumps.wikimedia.org/ltwiki/latest/ltwiki-latest-pages-articles.xml.bz2 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk3gF1c2DK6r"
      },
      "source": [
        "!pip install wikiextractor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZW-hOK-DQdO"
      },
      "source": [
        "!python3 -m wikiextractor.WikiExtractor -o wiki-lt/ --no-templates --processes 8 ltwiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHOpbhy9DcY1"
      },
      "source": [
        "!du; ls -s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsV07KJVmdeu"
      },
      "source": [
        "!cat wiki*/*/* > final_untokenised.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thUUHf9cmzzK"
      },
      "source": [
        "!cat wiki*/AA/wiki_*0* > tokentrain_final.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOY4WI1e8jSi"
      },
      "source": [
        "# Training the tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyZMOJpV7mdJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw9MSCZ4HG_H"
      },
      "source": [
        "!pip install youtokentome & pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgujED-ZHMR5"
      },
      "source": [
        "import youtokentome as yttm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oz9LeDLHO_C"
      },
      "source": [
        "yttm.BPE.train('/content/tokentrain_final.txt', 'model', 50000, 0.9999, n_threads=-1, pad_id=0, unk_id=1, bos_id=2, eos_id=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObpcRZKk84X"
      },
      "source": [
        "!yttm vocab --model model > vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdnfM95MlBV0"
      },
      "source": [
        "!sed 's/^ *[0-9]\\+.//g' /content/vocab.txt > /content/cleanvocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0oIPNxflDJW"
      },
      "source": [
        "text_file = open(\"/content/cleanvocab.txt\", \"r\")\n",
        "lines = text_file.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_03bKYlEX5"
      },
      "source": [
        "lines = [w.replace('‚ñÅ', ' ') for w in lines]\n",
        "print(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zc-TwZ38fia"
      },
      "source": [
        "# Encoding texts and making a tokeniser.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH7aM77mXRwy"
      },
      "source": [
        "initial = '{\"version\":\"1.0\",\"truncation\":null,\"padding\":null,\"added_tokens\":[{\"id\":50256,\"special\":true,\"content\":\"<|endoftext|>\",\"single_word\":false,\"lstrip\":false,\"rstrip\":false,\"normalized\":true}],\"normalizer\":null,\"pre_tokenizer\":{\"type\":\"ByteLevel\",\"add_prefix_space\":false,\"trim_offsets\":true},\"post_processor\":{\"type\":\"ByteLevel\",\"add_prefix_space\":true,\"trim_offsets\":false},\"decoder\":{\"type\":\"ByteLevel\",\"add_prefix_space\":true,\"trim_offsets\":true},\"model\":{\"dropout\":null,\"unk_token\":null,\"continuing_subword_prefix\":\"\",\"end_of_word_suffix\":\"\",\"fuse_unk\":false,\"vocab\":{'\n",
        "i = 0\n",
        "for token in lines:\n",
        "    if '\\\\' in token:\n",
        "       token = token.replace('\\\\', '\\\\\\\\')\n",
        "    if '\"' in token:\n",
        "       token = token.replace('\"', '\\\\\"')\n",
        "    initial = initial + '\"' + token + '\":' + str(i) + ','\n",
        "    i = i + 1\n",
        "initial = initial[:-1]\n",
        "initial = initial + '},\"merges\":[]}}'\n",
        "print(initial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-TBr27fbqVr"
      },
      "source": [
        "with open(\"aitextgen.tokeniser.json\", \"w\") as text_file:\n",
        "    text_file.write(initial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSz6GneDHVU7"
      },
      "source": [
        "!yttm encode --model /content/model --n_threads 8 --dropout_prob 0.25 --output_type id < /content/final_untokenised.txt > final.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4vSAGA__E41"
      },
      "source": [
        "# If the above doesn't work\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSHAkuEu_Jav"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "# initialize\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "# and train\n",
        "tokenizer.train(files='/content/your_dataset.txt', vocab_size=17000, min_frequency=0, special_tokens=lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7kIucJZ_KEN"
      },
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-qvdAh8YMZ"
      },
      "source": [
        "# Saving as npy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFhv0JywBX_5"
      },
      "source": [
        "There may be a more efficient way of doing this. I tried using linecache to read specific lines and then multithreaded it but then np.save broke, I'm not sure why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7W49E7Q_TQO"
      },
      "source": [
        "data_array = np.array([], dtype=np.uint16)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aMfz7o_VE8"
      },
      "source": [
        "np.save('data.npy', data_array)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USI8VJ22lYoM"
      },
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "file = open(\"translations_tokenised_new.txt\")\n",
        "i = 0\n",
        "while 1:\n",
        "    i = i + 50000000\n",
        "    string_data = file.read(50000000)\n",
        "    if not string_data:\n",
        "        break\n",
        "    list_data = re.split(' |\\n',string_data)\n",
        "    list_data = list(filter(None, list_data))\n",
        "    arr = np.array(list_data, dtype=np.uint16)\n",
        "    f_handle = open('data.npy', 'ab')\n",
        "    np.save(f_handle, arr)\n",
        "    f_handle.close()\n",
        "    print('Line ' + str(i) + ' done!')\n",
        "arr = []\n",
        "string_data = \"\"\n",
        "list_data = \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJv5TFgmpEu"
      },
      "source": [
        "!gzip data.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi3iG7zVltHq"
      },
      "source": [
        "!cp data.npy.gz /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
