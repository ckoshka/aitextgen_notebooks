{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoding very large text files - clean, final",
      "provenance": [],
      "collapsed_sections": [
        "TQ-qvdAh8YMZ"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_SqIOlD8yd7"
      },
      "source": [
        "# Optional: downloading training texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpeJ3Wup82qC"
      },
      "source": [
        "!pip install dtrx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQuu91lr_j4E"
      },
      "source": [
        "!lang=\"vi\"; wget -c https://dumps.wikimedia.org/${lang}wiki/latest/${lang}wiki-latest-pages-articles.xml.bz2 & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-wikimedia/v20210402/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-CCAligned/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-XLEnt/v1.1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-JW300/v1b/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-TED2020/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-GNOME/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-bible-uedin/v1/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-ParaCrawl/v8/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-TildeMODEL/v2018/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-DGT/v2019/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-EUbookshop/v2/mono/${lang}.txt.gz & wget https://object.pouta.csc.fi/OPUS-SETIMES/v2/mono/${lang}.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBnbucariRJh"
      },
      "source": [
        "!wget https://dumps.wikimedia.org/lzhwiki/latest/lzhwiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPvrtGy9AW5n"
      },
      "source": [
        "!dtrx *.txt.gz.* & dtrx *.txt.gz & dtrx *.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFasaOzpBCrT"
      },
      "source": [
        "!cat *.txt > dataset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMWzd0QS-c4h"
      },
      "source": [
        "# Downloading and cleaning Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhH-lihUBIl5"
      },
      "source": [
        "!nohup curl -L -O https://github.com/cltk/sanskrit_text_gitasupersite.git &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk3gF1c2DK6r"
      },
      "source": [
        "!pip install wikiextractor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZW-hOK-DQdO"
      },
      "source": [
        "!lang=\"lij\"; python3 -m wikiextractor.WikiExtractor -o wiki-${lang}/ --no-templates --processes 8 /content/${lang}wiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u59iIwSDatdr"
      },
      "source": [
        "truncate -s 35M *.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHOpbhy9DcY1"
      },
      "source": [
        "!du; ls -s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W8AxDHqUUmE"
      },
      "source": [
        "!wget https://github.com/mathisve/LatinTextDataset/blob/master/latincorpus.txt?raw=true & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/pt.txt.gz & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/gl.txt.gz & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/mwl.txt.gz & wget https://object.pouta.csc.fi/OPUS-TED2020/v1/mono/es.txt.gz & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/an.txt.gz & wget https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/mono/ca.txt.gz & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/oc.txt.gz & wget https://object.pouta.csc.fi/OPUS-GlobalVoices/v2018q4/mono/fr.txt.gz & wget https://object.pouta.csc.fi/OPUS-wikimedia/v20210402/mono/ast.txt.gz & wget https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/mono/rm.txt.gz & git clone https://github.com/ProSvizraRumantscha/corpora & wget https://object.pouta.csc.fi/OPUS-GNOME/v1/mono/fur.txt.gz & wget https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/mono/fur.txt.gz & wget https://object.pouta.csc.fi/OPUS-Ubuntu/v14.10/mono/fur.txt.gz & wget https://dumps.wikimedia.org/furwiki/latest/furwiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/pmswiki/latest/pmswiki-latest-pages-articles.xml.bz2 & wget https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/mono/la.txt.gz & wget https://dumps.wikimedia.org/lijwiki/latest/lijwiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/lmowiki/latest/lmowiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/emlwiki/latest/emlwiki-latest-pages-articles.xml.bz2 & wget https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/it.txt.gz & wget https://dumps.wikimedia.org/emlwiki/latest/emlwiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/scnwiki/latest/scnwiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/napwiki/latest/napwiki-latest-pages-articles.xml.bz2 & wget https://dumps.wikimedia.org/scwiki/latest/scwiki-latest-pages-articles.xml.bz2 & wget https://object.pouta.csc.fi/OPUS-wikimedia/v20210402/mono/sc.txt.gz & wget https://object.pouta.csc.fi/OPUS-QED/v2.0a/mono/ro.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOY4WI1e8jSi"
      },
      "source": [
        "# Training the tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyZMOJpV7mdJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXFPA8yqdnmS"
      },
      "source": [
        "!cp -r /content/sino3encode.txt /content/gdrive/MyDrive/language_groups/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcfWWtPS7P_k"
      },
      "source": [
        "!cp /content/romanceall.txt /content/gdrive/MyDrive/language_groups/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBy4ILrP5Su2"
      },
      "source": [
        "!cp /content/gdrive/MyDrive/language_groups/germanic/da_part_1.txt.gz /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw9MSCZ4HG_H"
      },
      "source": [
        "!pip install youtokentome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgujED-ZHMR5"
      },
      "source": [
        "import youtokentome as yttm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oz9LeDLHO_C"
      },
      "source": [
        "yttm.BPE.train('/content/romance.txt', 'romance_model_new', 35000, 0.999, n_threads=-1, pad_id=0, unk_id=1, bos_id=2, eos_id=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObpcRZKk84X"
      },
      "source": [
        "!yttm vocab --model romance_model_new > vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdnfM95MlBV0"
      },
      "source": [
        "!sed 's/^ *[0-9]\\+.//g' /content/vocab.txt > /content/cleanvocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0oIPNxflDJW"
      },
      "source": [
        "text_file = open(\"/content/cleanvocab.txt\", \"r\")\n",
        "lines = text_file.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rO9Aij--e8H"
      },
      "source": [
        "import unicodedata\n",
        "def remove_control_characters(s):\n",
        "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_03bKYlEX5"
      },
      "source": [
        "lines = [remove_control_characters(w.replace('‚ñÅ', ' ')) for w in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zc-TwZ38fia"
      },
      "source": [
        "# Making a tokeniser.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH7aM77mXRwy"
      },
      "source": [
        "initial = '{\"version\":\"1.0\",\"truncation\":null,\"padding\":null,\"added_tokens\":[{\"id\":50256,\"special\":true,\"content\":\"<|endoftext|>\",\"single_word\":false,\"lstrip\":false,\"rstrip\":false,\"normalized\":true}],\"normalizer\":null,\"pre_tokenizer\":{\"type\":\"ByteLevel\",\"add_prefix_space\":false,\"trim_offsets\":true},\"post_processor\":{\"type\":\"ByteLevel\",\"add_prefix_space\":true,\"trim_offsets\":false},\"decoder\":{\"type\":\"ByteLevel\",\"add_prefix_space\":true,\"trim_offsets\":true},\"model\":{\"dropout\":null,\"unk_token\":null,\"continuing_subword_prefix\":\"\",\"end_of_word_suffix\":\"\",\"fuse_unk\":false,\"vocab\":{'\n",
        "i = 0\n",
        "for token in lines:\n",
        "    if token == \"\":\n",
        "      token = str(i) + \"(#@&#*\"\n",
        "    if '\\\\' in token:\n",
        "       token = token.replace('\\\\', '\\\\\\\\')\n",
        "    if '\"' in token:\n",
        "       token = token.replace('\"', '\\\\\"')\n",
        "    stuff = '\"' + token + '\":' + str(i) + ','\n",
        "    initial+=stuff\n",
        "    i = i + 1\n",
        "initial = initial[:-1]\n",
        "initial = initial + '},\"merges\":[]}}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-TBr27fbqVr"
      },
      "source": [
        "with open(\"aitextgen_romance.tokeniser.json\", \"w\") as text_file:\n",
        "    text_file.write(initial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heIkb11XsRwT"
      },
      "source": [
        "# Encoding texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mARAHvrOcrLZ"
      },
      "source": [
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2dKgJk5sUaX"
      },
      "source": [
        "textList = os.listdir('/texts')\n",
        "random.shuffle(textList)\n",
        "textList = str(textList).replace(\"', '\", \" \").replace(\"['\", \"\").replace(\"']\", \"\")\n",
        "textList = \"cat \" + textList + \" > shuffled.txt\"\n",
        "os.system(textList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etQgSIHbchaM"
      },
      "source": [
        "os.system(\"mkdir /shuffled\")\n",
        "os.system(\"split -C 30M germanicall.txt /shuffled/_shuffle\")\n",
        "textList = os.listdir('/shuffled')\n",
        "random.shuffle(textList)\n",
        "textList = str(textList).replace(\"', '\", \" /shuffled/\").replace(\"['\", \"\").replace(\"']\", \"\")\n",
        "textList = \"cat \" + \"/shuffled/\" + textList + \" > shuffledagain_germanic.txt\"\n",
        "os.system(textList)\n",
        "os.system(\"rm /shuffled/*\")\n",
        "print(textList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSz6GneDHVU7"
      },
      "source": [
        "!yttm encode --model germanic_model_new --n_threads -1 --dropout_prob 0.22 --output_type id < shuffledagain_germanic.txt > final_germanic_encoded.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4vSAGA__E41"
      },
      "source": [
        "# If the above doesn't work\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSHAkuEu_Jav"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "# initialize\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "# and train\n",
        "tokenizer.train(files='/content/your_dataset.txt', vocab_size=17000, min_frequency=0, special_tokens=lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7kIucJZ_KEN"
      },
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-qvdAh8YMZ"
      },
      "source": [
        "# Saving as npy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFhv0JywBX_5"
      },
      "source": [
        "There may be a more efficient way of doing this. I tried using linecache to read specific lines and then multithreaded it but then np.save broke, I'm not sure why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuk33LLMbOSS"
      },
      "source": [
        "newfp = np.memmap('final.array', dtype=np.uint16, mode='w+', shape=(1,1)) # 38.1MB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USI8VJ22lYoM"
      },
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "for x in range(0, 20):\n",
        "    list_data = []\n",
        "    file = open('sino_encoded.txt')\n",
        "    fp = newfp\n",
        "    shape = fp.shape\n",
        "    data = np.array((list(filter(str.isdigit(), re.split(' |\\n',file.read(450000000))))), dtype=np.uint16)\n",
        "    if data.size == 0:\n",
        "        fp.flush()\n",
        "        break\n",
        "    data_shape = data.shape\n",
        "    concat_shape = data_shape[:-1] + (data_shape[-1] + shape[-1],)\n",
        "    print('cancat shape:{}'.format(concat_shape))\n",
        "    newfp = np.memmap('final.array', dtype=np.uint16, mode='r+', shape=concat_shape)\n",
        "    if len(concat_shape) == 1:\n",
        "        newfp[:shape[0]] = fp[:]\n",
        "        newfp[shape[0]:] = data[:]\n",
        "    if len(concat_shape) == 2:\n",
        "        newfp[:, :shape[-1]] = fp[:]\n",
        "        newfp[:, shape[-1]:] = data[:]\n",
        "    elif len(concat_shape) == 3:\n",
        "        newfp[:, :, :shape[-1]] = fp[:]\n",
        "        newfp[:, :, shape[-1]:] = data[:]\n",
        "    fp = newfp\n",
        "    fp.flush()\n",
        "    print(str(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNudl53taBWI"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.lib.format import open_memmap\n",
        "\n",
        "# a 10GB memory-mapped array\n",
        "x = np.memmap('final.array', mode='r', dtype=np.uint16, shape=(int(1E10),))\n",
        "p = open_memmap('final.npy', mode='w+', dtype=np.uint16, shape=x.shape)\n",
        "\n",
        "# copy the array contents\n",
        "p[:] = x[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5IMXVjWbDJk"
      },
      "source": [
        "# Alternative chunking method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLGz-4ykbgrh"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "file_name = \"final_germanic_encoded.txt\"\n",
        "file = open(file_name)\n",
        "for x in range(0, 40):\n",
        "    numpyPrefix = \"/content/gdrive/MyDrive/language_groups/data_romance_\" + str(x) + \".npy\"\n",
        "    list_data = (list(filter(None, re.split(' |\\n',file.read(900000000)))))\n",
        "    if len(list_data) == 0:\n",
        "        break\n",
        "    np.save(numpyPrefix, np.array(list_data, dtype=np.uint16))\n",
        "    string_data = \"\"\n",
        "    list_data = []\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpADBenfbhMF"
      },
      "source": [
        "# Saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMJv5TFgmpEu"
      },
      "source": [
        "!gzip final.npy; cp final.npy.gz /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
